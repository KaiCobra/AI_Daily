# AI Daily: FlexVAR - 擺脫殘差預測，實現靈活高效的視覺自回歸生成

> **論文名稱**: FlexVAR: Flexible Visual Autoregressive Modeling without Residual Prediction
> **機構**: 北京交通大學、悉尼科技大學、美團、喬治亞理工學院
> **作者**: Siyu Jiao, Gengwei Zhang, Yinlong Qian, 等
> **發表時間**: 2026年1月12日
> **論文連結**: [https://arxiv.org/abs/2502.20313](https://arxiv.org/abs/2502.20313)

---

## 一、核心貢獻：挑戰傳統，擁抱靈活性

近年來，視覺自回歸模型（Visual Autoregressive, VAR）在圖像生成領域取得了顯著進展，其核心思想是採用**尺度遞增（scale-wise）**的方式逐步生成圖像。然而，以 `VAR` 為代表的傳統模型普遍依賴於一種**殘差預測（residual prediction）**範式：模型在第一步預測一個低解析度的基礎圖像，在後續的每一步中，則預測當前尺度與前一尺度之間的「差異」或「殘差」。

這種設計雖然有效，卻帶來了兩個核心問題：
1.  **剛性流程**：生成過程被鎖定在固定的步驟和解析度中，難以適應不同的生成需求。
2.  **語義模糊**：殘差本身缺乏直觀的語義信息，可能限制了模型捕捉圖像多樣性的能力。

為了解決這些痛點，**FlexVAR** 應運而生。它徹底拋棄了殘差預測，提出了一種更為直觀和靈活的**全局真值預測（Ground-Truth Prediction）**新範式。在 FlexVAR 的框架下，自回歸模型的每一步都直接預測該尺度下的**完整圖像內容（GT）**，而非僅僅是與上一尺度之間的差異。

![VAR vs FlexVAR 架構對比](asset/flexvar_architecture_diagram.webp)
*圖一：左側為傳統 VAR 的殘差預測流程，右側為 FlexVAR 的全局真值預測流程。FlexVAR 的每一步都生成一個語義完整的圖像。*

這一看似簡單的改變，卻為視覺自回歸模型帶來了前所未有的**靈活性**和**高效性**。僅在低解析度（≤ 256px）圖像上訓練的 FlexVAR，便能實現：

- **任意解析度與長寬比生成**：能夠零樣本（zero-shot）生成遠超訓練解析度的圖像。
- **多功能圖像編輯**：無需額外訓練，即可支援圖像修復、擴展、內外繪製等任務。
- **可變推理步驟**：使用者可以根據需求，在「速度」與「品質」之間自由權衡，用更少的步驟加速推理，或用更多的步驟提升細節。

## 二、技術方法簡述

FlexVAR 的成功主要歸功於以下幾個關鍵設計：

### 1. 全局真值預測（Ground-Truth Prediction）

這是 FlexVAR 的核心思想。模型不再學習模糊的殘差，而是直接學習從一系列低解析度圖像到目標高解析度圖像的映射。其自回歸概率分佈被重新定義為：

$$ p(g_1, g_2, \dots, g_n) = \prod_{i=1}^{n} p(g_i \mid g_1, g_2, \dots, g_{i-1}) $$

其中，$g_i$ 代表第 $i$ 個尺度的圖像 token map。在每一步，模型都基於之前生成的所有尺度的圖像，並行預測當前尺度的所有 tokens，生成一個語義完整的 $g_i$。

### 2. 可擴展的 VQVAE 與 2D 位置嵌入

為了配合全局真值預測，FlexVAR 進行了兩項重要改進：

- **可擴展 VQVAE (Scalable VQVAE)**：傳統 VQVAE 在處理其訓練之外的潛在尺度時，重建效果會急劇下降。FlexVAR 設計了一個對多尺度輸入更具魯棒性的 VQVAE，使其能夠在任意解析度下進行可靠的圖像量化與重建。

- **可擴展 2D 位置嵌入 (Scalable 2D PE)**：傳統 VAR 的位置嵌入長度固定，限制了其靈活性。FlexVAR 則設計了一種可動態縮放的 2D 位置嵌入，並移除了固定的步驟嵌入（step embeddings），使得模型能夠在訓練和推理時處理任意數量和尺寸的尺度，這是實現零樣本解析度擴展的關鍵。

### 3. 靈活的步驟採樣策略

在訓練過程中，FlexVAR 採用了隨機的步驟採樣策略。除了固定起始和結束尺度外，中間的尺度大小和步驟數量都是隨機變化的。這種策略極大地增強了模型對不同生成路徑的適應能力，使其在推理時能夠靈活應對不同的步驟數和解析度組合。

## 三、實驗結果與分析

FlexVAR 在標準的 ImageNet 256x256 圖像生成基準測試中取得了卓越的性能，全面超越了同等規模的 VAR 模型。

| 模型 | FID (↓) | IS (↑) | 參數量 | 推理步驟 |
|:---|:---:|:---:|:---:|:---:|
| VAR-d24 | 2.33 | 312.9 | 1.0B | 10 |
| **FlexVAR-d24** | **2.21** | 299.1 | 1.0B | 10 |
| **FlexVAR-d24 (13步)** | **2.08** | **315.7** | 1.0B | 13 |

從上表可見，僅用 10 個推理步驟，1.0B 參數的 FlexVAR-d24 在 FID 指標上就已優於 VAR-d24。當推理步驟增加到 13 步時，其 FID 進一步降低至 **2.08**，不僅顯著優於 AiM (2.56) 和 VAR (2.36)，甚至超越了主流的擴散模型 DiT-XL/2 (2.27)。

更令人印象深刻的是其**零樣本泛化能力**。

![FlexVAR 生成樣本](asset/flexvar_generated_samples.webp)
*圖二：FlexVAR-d24 (1.0B) 生成的樣本，儘管僅在 256x256 解析度下訓練，卻能生成各種解析度和長寬比的高品質圖像。*

一個僅在 256x256 解析度上訓練的 1.0B FlexVAR 模型，能夠直接在 512x512 解析度的基準上，取得與一個在 512x512 解析度上進行**完全監督訓練**的 2.3B VAR 模型相媲美的結果。這充分證明了 FlexVAR 設計的優越性和強大的泛化潛力。

## 四、個人評價與意義

FlexVAR 的提出，為視覺自回歸模型的研究開闢了一條新的道路。它證明了**殘差預測並非 VAR 模型的必需品**，更直觀的全局真值預測範式不僅可行，而且能帶來巨大的靈活性和性能提升。

這項工作完美契合了當前 AI 生成領域對於**高效、靈活、可控**模型的追求。對於希望激發 `VAR-based` 模型潛力、探索 `training-free` 推理策略（如動態調整步驟和解析度）以及提升模型**注意力調製（attention modulation）**效率的研究者來說，FlexVAR 提供了一個極具啟發性的範例。

通過擺脫剛性的殘差依賴，FlexVAR 讓自回歸模型朝著像 GPT 系列在 NLP 領域那樣的通用視覺生成基礎模型邁出了堅實的一步。我們可以期待，未來基於 FlexVAR 思想的模型將在文生圖、影片生成和多模態任務中扮演更重要的角色。

---

**參考文獻**

[1] Jiao, S., Zhang, G., Qian, Y., Huang, J., Zhao, Y., Shi, H., Ma, L., & Jie, Z. (2026). *FlexVAR: Flexible Visual Autoregressive Modeling without Residual Prediction*. arXiv preprint arXiv:2502.20313.
